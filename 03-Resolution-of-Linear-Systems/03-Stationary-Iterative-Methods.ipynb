{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Stationary Iterative Methods\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "While direct methods like Gaussian Elimination and LU Decomposition provide an exact solution in a finite number of steps (ignoring round-off errors), **iterative methods** offer a different approach. They start with an initial guess for the solution and progressively refine it through a series of iterations until it is hopefully \"close enough\" to the true solution.\n",
    "\n",
    "These methods are particularly powerful for very **large and sparse systems** (matrices with many zero entries), where direct methods would be computationally too expensive or would require too much memory.\n",
    "\n",
    "A **stationary iterative method** has the general form:\n",
    "$$ x^{(k+1)} = Bx^{(k)} + c $$\n",
    "Where $x^{(k)}$ is the solution vector at the $k$-th iteration. The method is called \"stationary\" because the iteration matrix $B$ and the vector $c$ remain **fixed** throughout the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Infinity Norm and Stopping Criterion\n",
    "\n",
    "To measure the difference between successive solution vectors and decide when to stop, we need a way to determine the \"size\" or \"magnitude\" of a vector. A common choice is the **infinity norm** ($L_\\infty$-norm).\n",
    "\n",
    "The infinity norm of a vector $x$ is simply the maximum absolute value of its components:\n",
    "$$ ||x||_\\infty = \\max_{1 \\le i \\le n} |x_i| $$\n",
    "\n",
    "#### Stopping Criterion\n",
    "Our primary stopping criterion will be the **relative error** between two consecutive iterations, measured using the infinity norm. We stop when this error falls below a given tolerance $\\epsilon$.\n",
    "$$ \\frac{||x^{(k)} - x^{(k-1)}||_\\infty}{||x^{(k)}||_\\infty} < \\epsilon $$\n",
    "\n",
    "To prevent infinite loops in cases of divergence, we must also set a **maximum number of iterations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Jacobi Method\n",
    "\n",
    "The Jacobi method is one of the simplest iterative techniques. The core idea is to rearrange each equation in the system $Ax=b$ to isolate one variable.\n",
    "\n",
    "Starting with the system:\n",
    "$$ \n",
    "\\begin{cases}\n",
    "a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1 \\\\\n",
    "a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2 \\\\\n",
    "\\vdots \\\\\n",
    "a_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n = b_n\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We can isolate each variable $x_i$ (assuming $a_{ii} \\neq 0$):\n",
    "$$ x_i = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j \\neq i} a_{ij}x_j \\right) $$\n",
    "\n",
    "This rearrangement forms the basis of the iterative process. To get the next approximation $x^{(k)}$, we plug the values from the *entire* previous approximation $x^{(k-1)}$ into the right-hand side of the equations:\n",
    "$$ x_i^{(k)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j \\neq i} a_{ij}x_j^{(k-1)} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence Analysis: The Row-Sum Criterion\n",
    "A key question for any iterative method is: will it converge? A sufficient (but not necessary) condition for convergence is the **row-sum criterion**, which is met if the matrix is **strictly diagonally dominant**.\n",
    "\n",
    "A matrix is strictly diagonally dominant if, for every row, the absolute value of the diagonal element is greater than the sum of the absolute values of all other elements in that row.\n",
    "\n",
    "Formally, we define for each row $i$:\n",
    "$$ \\rho_i = \\sum_{\\substack{j=1 \\\\ j \\ne i}}^n \\left| \\frac{a_{ij}}{a_{ii}} \\right| $$\n",
    "\n",
    "If $\\rho = \\max_i(\\rho_i) < 1$, then the Jacobi method is **guaranteed to converge** for any initial guess $x^{(0)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Jacobi Method: Implementation and Example\n",
    "\n",
    "**Example System:**\n",
    "$$ \n",
    "\\begin{cases}\n",
    "4x_1 + 0.24x_2 - 0.08x_3 = 8 \\\\\n",
    "0.03x_1 + 3x_2 - 0.15x_3 = 9 \\\\\n",
    "0.04x_1 - 0.08x_2 + 4x_3 = 20\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def jacobi_method(A, b, x0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"Solves Ax=b using the Jacobi iterative method.\"\"\"\n",
    "    n = len(b)\n",
    "    x = x0.copy()\n",
    "    history = [x0.copy()]\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        x_prev = x.copy()\n",
    "        for i in range(n):\n",
    "            sigma = np.dot(A[i, :i], x_prev[:i]) + np.dot(A[i, i+1:], x_prev[i+1:])\n",
    "            x[i] = (b[i] - sigma) / A[i, i]\n",
    "        \n",
    "        history.append(x.copy())\n",
    "        \n",
    "        # Check for convergence\n",
    "        error = np.linalg.norm(x - x_prev, np.inf) / (np.linalg.norm(x, np.inf) + 1e-12)\n",
    "        if error < tol:\n",
    "            print(f\"Jacobi converged after {k+1} iterations.\")\n",
    "            return x, pd.DataFrame(history)\n",
    "    \n",
    "    print(\"Jacobi failed to converge within the maximum number of iterations.\")\n",
    "    return x, pd.DataFrame(history)\n",
    "\n",
    "# --- Setup for the example ---\n",
    "A_ex = np.array([\n",
    "    [4, 0.24, -0.08],\n",
    "    [0.03, 3, -0.15],\n",
    "    [0.04, -0.08, 4]\n",
    "])\n",
    "\n",
    "b_ex = np.array([8, 9, 20])\n",
    "x0_ex = np.zeros(3)\n",
    "\n",
    "# Check the convergence criterion\n",
    "diag = np.diag(np.abs(A_ex))\n",
    "off_diag = np.sum(np.abs(A_ex), axis=1) - diag\n",
    "if np.all(diag > off_diag):\n",
    "    print(\"Matrix is strictly diagonally dominant. Jacobi method will converge.\")\n",
    "else:\n",
    "    print(\"Matrix is not strictly diagonally dominant. Convergence is not guaranteed.\")\n",
    "\n",
    "# Solve the system\n",
    "jacobi_sol, jacobi_hist = jacobi_method(A_ex, b_ex, x0_ex, max_iter=3)\n",
    "print(\"\\nResults after 3 iterations:\")\n",
    "jacobi_hist.columns = ['x1', 'x2', 'x3']\n",
    "display(jacobi_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The Gauss-Seidel Method\n",
    "\n",
    "The Gauss-Seidel method is a simple but effective refinement of the Jacobi method. The key difference is that it uses the **most recently updated values** of the variables as soon as they become available within the same iteration.\n",
    "\n",
    "For example, when calculating $x_2^{(k)}$, it uses the newly computed $x_1^{(k)}$ instead of the old $x_1^{(k-1)}$. This generally leads to a faster rate of convergence.\n",
    "\n",
    "The iterative formula becomes:\n",
    "$$ x_i^{(k)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j < i} a_{ij}x_j^{(k)} - \\sum_{j > i} a_{ij}x_j^{(k-1)} \\right) $$\n",
    "\n",
    "The convergence criterion (strict diagonal dominance) is the same as for the Jacobi method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Gauss-Seidel: Implementation and Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_seidel_method(A, b, x0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"Solves Ax=b using the Gauss-Seidel iterative method.\"\"\"\n",
    "    n = len(b)\n",
    "    x = x0.copy()\n",
    "    history = [x0.copy()]\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        x_prev = x.copy()\n",
    "        for i in range(n):\n",
    "            # Note the use of the updated x vector inside the loop\n",
    "            sigma = np.dot(A[i, :i], x[:i]) + np.dot(A[i, i+1:], x_prev[i+1:])\n",
    "            x[i] = (b[i] - sigma) / A[i, i]\n",
    "        \n",
    "        history.append(x.copy())\n",
    "        \n",
    "        # Check for convergence\n",
    "        error = np.linalg.norm(x - x_prev, np.inf) / (np.linalg.norm(x, np.inf) + 1e-12)\n",
    "        if error < tol:\n",
    "            print(f\"Gauss-Seidel converged after {k+1} iterations.\")\n",
    "            return x, pd.DataFrame(history)\n",
    "    \n",
    "    print(\"Gauss-Seidel failed to converge within the maximum number of iterations.\")\n",
    "    return x, pd.DataFrame(history)\n",
    "\n",
    "# Solve the same system with Gauss-Seidel\n",
    "gs_sol, gs_hist = gauss_seidel_method(A_ex, b_ex, x0_ex, max_iter=3)\n",
    "print(\"\\nResults after 3 iterations:\")\n",
    "gs_hist.columns = ['x1', 'x2', 'x3']\n",
    "display(gs_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Comparison and Summary\n",
    "\n",
    "Let's solve the system to convergence and compare the performance of the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Solve to convergence\n",
    "jacobi_sol_final, jacobi_hist_final = jacobi_method(A_ex, b_ex, x0_ex, tol=1e-8)\n",
    "gs_sol_final, gs_hist_final = gauss_seidel_method(A_ex, b_ex, x0_ex, tol=1e-8)\n",
    "\n",
    "print(\"--- Final Solutions ---\")\n",
    "print(f\"Jacobi:       {jacobi_sol_final}\")\n",
    "print(f\"Gauss-Seidel: {gs_sol_final}\")\n",
    "\n",
    "# Calculate the true solution for error comparison\n",
    "true_sol = np.linalg.solve(A_ex, b_ex)\n",
    "\n",
    "# Calculate error at each iteration\n",
    "jacobi_errors = [np.linalg.norm(x - true_sol, np.inf) for x in jacobi_hist_final.values]\n",
    "gs_errors = [np.linalg.norm(x - true_sol, np.inf) for x in gs_hist_final.values]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(jacobi_errors, 'o-', label='Jacobi Error')\n",
    "plt.semilogy(gs_errors, 's-', label='Gauss-Seidel Error')\n",
    "plt.title('Convergence Comparison: Jacobi vs. Gauss-Seidel')\n",
    "plt.xlabel('Iteration (k)')\n",
    "plt.ylabel('Error (Infinity Norm)')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot clearly shows that for this (and many other) problems, **Gauss-Seidel converges significantly faster** than Jacobi. The error drops much more steeply per iteration.\n",
    "\n",
    "| Property                  | **Jacobi** | **Gauss-Seidel** |\n",
    "|---------------------------|---------------------------------------------|------------------------------------------------|\n",
    "| **Core Idea** | Uses only values from the previous iteration. | Uses the newest available values immediately.    |\n",
    "| **Convergence Speed** | Generally slower.                           | Generally faster.                              |\n",
    "| **Parallelization** | Easy to parallelize (all updates are independent). | Difficult to parallelize (updates are sequential). |\n",
    "| **Implementation** | Requires two vectors (current and previous).  | Can be implemented \"in-place\" with one vector.   |\n",
    "| **Convergence Condition** | Strict diagonal dominance is sufficient.    | Strict diagonal dominance is sufficient.       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}