{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Newton-Raphson Method\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction and Geometric Intuition\n",
    "\n",
    "The **Newton-Raphson Method**, or simply **Newton's Method**, is one of the most powerful and well-known numerical methods for finding the root of a function. Unlike bracketing methods, it doesn't require an interval that contains the root. Instead, it starts with a single initial guess, $x_0$, and iteratively improves it.\n",
    "\n",
    "The core idea is geometric: at each step, we approximate the function with its **tangent line** at the current guess. The next guess for the root is then the point where this tangent line intersects the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 4\n",
    "\n",
    "def df(x):\n",
    "    return 2*x\n",
    "\n",
    "x0 = 6.0\n",
    "x_vals = np.linspace(0, 7, 400)\n",
    "y_vals = f(x_vals)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_vals, y_vals, label='$f(x) = x^2 - 4$')\n",
    "plt.axhline(0, color='black', lw=0.5)\n",
    "\n",
    "# First iteration\n",
    "y0 = f(x0)\n",
    "tangent_1 = df(x0) * (x_vals - x0) + y0\n",
    "x1 = x0 - f(x0)/df(x0)\n",
    "plt.plot(x_vals, tangent_1, 'r--', label=f'Tangent at $x_0={x0}$')\n",
    "plt.scatter([x0, x1], [0, 0], color='red', zorder=5)\n",
    "plt.text(x0, -5, f'$x_0$', fontsize=12)\n",
    "plt.text(x1, -5, f'$x_1$', fontsize=12)\n",
    "\n",
    "# Second iteration\n",
    "y1 = f(x1)\n",
    "tangent_2 = df(x1) * (x_vals - x1) + y1\n",
    "x2 = x1 - f(x1)/df(x1)\n",
    "plt.plot(x_vals, tangent_2, 'g--', label=f'Tangent at $x_1={x1:.2f}$')\n",
    "plt.scatter(x2, 0, color='green', zorder=5)\n",
    "plt.text(x2, -5, f'$x_2$', fontsize=12)\n",
    "\n",
    "plt.title('Geometric Intuition of the Newton-Raphson Method')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(-10, 35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Derivation of the Formula\n",
    "\n",
    "We can derive the method's formula from the equation of a tangent line. The tangent line to the function $f(x)$ at a point $x_k$ is given by:\n",
    "$$ y - f(x_k) = f'(x_k) (x - x_k) $$\n",
    "\n",
    "We want to find where this line crosses the x-axis, so we set $y=0$ and solve for $x$, which will be our next approximation, $x_{k+1}$:\n",
    "$$ 0 - f(x_k) = f'(x_k) (x_{k+1} - x_k) $$\n",
    "$$ -f(x_k) = f'(x_k)x_{k+1} - f'(x_k)x_k $$\n",
    "$$ f'(x_k)x_{k+1} = f'(x_k)x_k - f(x_k) $$\n",
    "\n",
    "Assuming $f'(x_k) \\neq 0$, we can divide by it to get the famous **Newton-Raphson formula**:\n",
    "$$ x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Connection to Fixed-Point Iteration and Convergence\n",
    "\n",
    "Newton's method is actually a special, powerful case of the Fixed-Point Iteration method.\n",
    "\n",
    "If we define our iteration function $g(x)$ as:\n",
    "$$ g(x) = x - \\frac{f(x)}{f'(x)} $$\n",
    "\n",
    "Then the Newton-Raphson formula $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$ is equivalent to the fixed-point formula $x_{k+1} = g(x_k)$.\n",
    "\n",
    "#### Are convergence checks still necessary?\n",
    "\n",
    "**Yes, absolutely.** The idea that checks are no longer needed is a dangerous oversimplification. To understand why, let's analyze the convergence condition for this specific $g(x)$. The Fixed-Point Theorem states that we need $|g'(x)| < 1$ for convergence. Let's find the derivative of our $g(x)$ using the quotient rule:\n",
    "$$ g'(x) = \\frac{d}{dx} \\left( x - \\frac{f(x)}{f'(x)} \\right) = 1 - \\frac{f'(x)f'(x) - f(x)f''(x)}{[f'(x)]^2} = 1 - \\frac{[f'(x)]^2}{[f'(x)]^2} + \\frac{f(x)f''(x)}{[f'(x)]^2} $$\n",
    "$$ g'(x) = \\frac{f(x)f''(x)}{[f'(x)]^2} $$\n",
    "\n",
    "Now consider the value of this derivative at the actual root, $r$. Since $f(r) = 0$, we have:\n",
    "$$ g'(r) = \\frac{f(r)f''(r)}{[f'(r)]^2} = \\frac{0 \\cdot f''(r)}{[f'(r)]^2} = 0 $$\n",
    "\n",
    "This is a remarkable result. Since $g'(r) = 0$, which is much less than 1, the convergence is extremely fast (it is **quadratic convergence**), meaning the number of correct decimal places roughly doubles with each iteration.\n",
    "\n",
    "**However, this is only true if we are *already close* to the root.** If our initial guess $x_0$ is far from the root, or if we encounter a point where $f'(x_k) \\approx 0$, the method can fail spectacularly. The convergence condition still applies, and we are not exempt from checking it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Algorithm\n",
    "\n",
    "1.  **Input**: The function $f(x)$, its derivative $f'(x)$, an initial guess $x_0$, a tolerance $\\epsilon$, and a maximum number of iterations `max_iter`.\n",
    "2.  **Initialize**: Set a counter $k = 1$.\n",
    "3.  **Loop**: While $k \\le$ `max_iter`:\n",
    "    a. Check if $f'(x_{k-1})$ is close to zero. If so, the method may fail; stop and report an error.\n",
    "    b. Calculate the next approximation: $x_k = x_{k-1} - \\frac{f(x_{k-1})}{f'(x_{k-1})}$.\n",
    "    c. Check the stopping criterion (e.g., if $|x_k - x_{k-1}| < \\epsilon$). If met, stop.\n",
    "    d. Increment the counter: $k = k + 1$.\n",
    "4.  **Output**: Return the final approximation $x_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Python Implementation and Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def newton_raphson(f, df, x0, tol=1e-6, max_iter=50):\n",
    "    \"\"\"\n",
    "    Finds a root using the Newton-Raphson method.\n",
    "    \n",
    "    Returns:\n",
    "        The approximate root and a DataFrame of the iteration history.\n",
    "    \"\"\"\n",
    "    k = 0\n",
    "    xk = x0\n",
    "    history = []\n",
    "    \n",
    "    while k < max_iter:\n",
    "        fxk = f(xk)\n",
    "        dfxk = df(xk)\n",
    "        \n",
    "        # Check for potential division by zero\n",
    "        if abs(dfxk) < 1e-12:\n",
    "            print(\"Error: Derivative is close to zero. Newton's method fails.\")\n",
    "            return None, pd.DataFrame(history, columns=['k', 'x_k', 'f(x_k)', 'Error'])\n",
    "        \n",
    "        xk_next = xk - fxk / dfxk\n",
    "        error = abs(xk_next - xk)\n",
    "        history.append([k, xk, fxk, error])\n",
    "        \n",
    "        if error < tol:\n",
    "            break\n",
    "            \n",
    "        xk = xk_next\n",
    "        k += 1\n",
    "        \n",
    "    df = pd.DataFrame(history, columns=['k', 'x_k', 'f(x_k)', 'Error |x_{k+1}-x_k|'])\n",
    "    df.set_index('k', inplace=True)\n",
    "    return xk_next, df\n",
    "\n",
    "# --- Example: f(x) = x^3 - 9x + 3 ---\n",
    "def f_ex(x):\n",
    "    return x**3 - 9*x + 3\n",
    "\n",
    "def df_ex(x):\n",
    "    return 3*x**2 - 9\n",
    "\n",
    "# Find the root in the interval [0, 1] with an initial guess\n",
    "initial_guess = 0.5\n",
    "tolerance = 1e-8\n",
    "\n",
    "root_approx, history_df = newton_raphson(f_ex, df_ex, initial_guess, tol=tolerance)\n",
    "\n",
    "print(f\"--- Newton-Raphson for f(x) = x^3 - 9x + 3 with x0 = {initial_guess} ---\")\n",
    "display(history_df)\n",
    "if root_approx is not None:\n",
    "    print(f\"\\nRoot approximation: {root_approx:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the incredibly fast convergence. The error shrinks quadratically, reaching the desired tolerance in just a few iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Failure Cases for Newton's Method\n",
    "\n",
    "Despite its speed, Newton's method is not foolproof. Here are common scenarios where it can fail:\n",
    "\n",
    "1.  **Horizontal Tangent ($f'(x_k) = 0$)**: If the derivative at a guess is zero, the tangent line is horizontal and will never intersect the x-axis. Our formula involves division by $f'(x_k)$, leading to a division-by-zero error.\n",
    "2.  **Oscillation**: The sequence of guesses can enter an infinite cycle, bouncing between two or more values without converging. This often happens with functions that have certain symmetries.\n",
    "3.  **Divergence**: If the initial guess is too far from the root, the tangent lines can actually send the subsequent guesses further and further away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Failure Case: Oscillation\n",
    "# f(x) = x^3 - 2x + 2\n",
    "\n",
    "def f_fail(x):\n",
    "    return x**3 - 2*x + 2\n",
    "\n",
    "def df_fail(x):\n",
    "    return 3*x**2 - 2\n",
    "\n",
    "print(\"--- Failure Case: Oscillation with x0 = 0 ---\")\n",
    "root_fail, history_fail_df = newton_raphson(f_fail, df_fail, x0=0, max_iter=10)\n",
    "display(history_fail_df)\n",
    "print(\"\\nThe method bounces between 0 and 1 indefinitely.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advantages and Disadvantages\n",
    "\n",
    "#### Advantages:\n",
    "1.  **Extremely Fast Convergence**: When it works, it converges quadratically, which is much faster than the linear convergence of Bisection or Fixed-Point Iteration.\n",
    "\n",
    "#### Disadvantages:\n",
    "1.  **Not Guaranteed to Converge**: It can fail spectacularly if the initial guess is poor or if the function has problematic characteristics (e.g., a derivative near zero).\n",
    "2.  **Requires the Derivative**: You must be able to compute the analytical derivative $f'(x)$, which may not always be easy or possible.\n",
    "3.  **Can be Sensitive to Initial Guess**: A good starting point is often crucial for convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}